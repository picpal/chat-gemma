# AI Model - Gemma 3n í†µí•©

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

### í•µì‹¬ ê¸°ëŠ¥
- Gemma 3n ëª¨ë¸ ë¡œì»¬ ì‹¤í–‰
- ì˜¤í”„ë¼ì¸ í™˜ê²½ ì™„ë²½ ì§€ì›
- ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
- ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”
- íì‡„ë§ ë°°í¬ ì§€ì›

### ê¸°ìˆ  ìŠ¤íƒ
- **ëª¨ë¸**: Google Gemma 3n (E4B ê¶Œì¥)
- **ëŸ°íƒ€ì„**: Ollama
- **ì»¨í…Œì´ë„ˆ**: Docker
- **API**: REST API (Ollama Native)
- **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana (ì„ íƒ)

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ai-model/
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ Dockerfile           # Ollama ì»¨í…Œì´ë„ˆ ì •ì˜
â”‚   â”œâ”€â”€ models/             # ëª¨ë¸ íŒŒì¼ ì €ì¥
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ download.sh     # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”œâ”€â”€ backup.sh       # ëª¨ë¸ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
â”‚       â””â”€â”€ health.sh       # í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ ollama.yml         # Ollama ì„¤ì •
â”‚   â””â”€â”€ models.json        # ëª¨ë¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ docker-compose.yml      # ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare-offline.sh # ì˜¤í”„ë¼ì¸ ì¤€ë¹„
â”‚   â””â”€â”€ deploy.sh          # ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ monitoring/            # ëª¨ë‹ˆí„°ë§ ì„¤ì • (ì„ íƒ)
    â”œâ”€â”€ prometheus.yml
    â””â”€â”€ grafana/
```

## ğŸ¤– Gemma 3n ëª¨ë¸ ìƒì„¸

### ëª¨ë¸ íŠ¹ì§•
- **MatFormer ì•„í‚¤í…ì²˜**: ì¤‘ì²©ëœ ëª¨ë¸ êµ¬ì¡°ë¡œ ë™ì  íŒŒë¼ë¯¸í„° í™œì„±í™”
- **PLE (Per-Layer Embeddings)**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹¤í–‰
- **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤(ì¶”í›„), ë¹„ë””ì˜¤(ì¶”í›„) ì§€ì›
- **128K ì»¨í…ìŠ¤íŠ¸**: ê¸´ ëŒ€í™” ì²˜ë¦¬ ê°€ëŠ¥
- **140ê°œ ì–¸ì–´**: ë‹¤êµ­ì–´ ì§€ì›

### ëª¨ë¸ ë²„ì „ ì„ íƒ

| ëª¨ë¸ | ì‹¤ì œ í¬ê¸° | íš¨ê³¼ì  í¬ê¸° | ë©”ëª¨ë¦¬ ìš”êµ¬ | ìš©ë„ |
|------|----------|------------|------------|------|
| E2B  | 6B       | 2B         | 4GB        | ê²½ëŸ‰ í™˜ê²½ |
| E4B  | 8B       | 4B         | 8GB        | **ê¶Œì¥** |
| 12B  | 12B      | 12B        | 16GB       | ê³ ì„±ëŠ¥ |
| 27B  | 27B      | 27B        | 32GB+      | ìµœê³  ì„±ëŠ¥ |

### ê¶Œì¥ ì‚¬ì–‘
- **ìµœì†Œ**: 8GB RAM, 20GB Storage
- **ê¶Œì¥**: 16GB RAM, 50GB Storage, GPU (ì„ íƒ)
- **ìš´ì˜**: 32GB RAM, 100GB Storage, NVIDIA GPU

## ğŸ³ Docker êµ¬ì„±

### Ollama Dockerfile

```dockerfile
# ai-model/ollama/Dockerfile
FROM ollama/ollama:latest

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV OLLAMA_MODELS=/root/.ollama/models
ENV OLLAMA_NUM_PARALLEL=2
ENV OLLAMA_MAX_LOADED_MODELS=1
ENV OLLAMA_KEEP_ALIVE=5m

# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
COPY scripts/download.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/download.sh

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
COPY scripts/health.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/health.sh

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ë¹Œë“œ ì‹œ)
RUN /usr/local/bin/download.sh

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD /usr/local/bin/health.sh || exit 1

EXPOSE 11434

CMD ["serve"]
```

### Docker Compose

```yaml
# ai-model/docker-compose.yml
version: '3.8'

services:
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: gemma3n-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama/models
      - ./config/ollama.yml:/etc/ollama/config.yml:ro
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama/models
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - gemma-network

volumes:
  ollama-models:
    driver: local

networks:
  gemma-network:
    external: true
```

## ğŸ“œ ìŠ¤í¬ë¦½íŠ¸

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# ai-model/ollama/scripts/download.sh

echo "Downloading Gemma 3n models..."

# Gemma 3n E4B (ê¶Œì¥)
ollama pull gemma3n:e4b

# Gemma 3n E2B (ê²½ëŸ‰)
ollama pull gemma3n:e2b

# ëª¨ë¸ í™•ì¸
ollama list

echo "Model download complete!"
```

### ì˜¤í”„ë¼ì¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# ai-model/scripts/prepare-offline.sh

set -e

echo "=== Gemma 3n ì˜¤í”„ë¼ì¸ ë°°í¬ ì¤€ë¹„ ==="

# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
echo "1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
docker-compose up -d ollama
sleep 10
docker exec gemma3n-ollama ollama pull gemma3n:e4b
docker exec gemma3n-ollama ollama pull gemma3n:e2b

# 2. ëª¨ë¸ íŒŒì¼ ë°±ì—…
echo "2. ëª¨ë¸ íŒŒì¼ ë°±ì—… ì¤‘..."
docker exec gemma3n-ollama tar -czf /tmp/models.tar.gz /root/.ollama/models
docker cp gemma3n-ollama:/tmp/models.tar.gz ./models-backup.tar.gz

# 3. Docker ì´ë¯¸ì§€ ì €ì¥
echo "3. Docker ì´ë¯¸ì§€ ì €ì¥ ì¤‘..."
docker save -o gemma3n-ollama.tar ollama/ollama:latest

# 4. ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„±
echo "4. ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„± ì¤‘..."
tar -czf gemma3n-offline-package.tar.gz \
    gemma3n-ollama.tar \
    models-backup.tar.gz \
    docker-compose.yml \
    scripts/deploy.sh

echo "=== ì¤€ë¹„ ì™„ë£Œ! ==="
echo "ë°°í¬ íŒŒì¼: gemma3n-offline-package.tar.gz"
```

### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# ai-model/scripts/deploy.sh

set -e

echo "=== Gemma 3n ì˜¤í”„ë¼ì¸ ë°°í¬ ==="

# 1. Docker ì´ë¯¸ì§€ ë¡œë“œ
echo "1. Docker ì´ë¯¸ì§€ ë¡œë“œ ì¤‘..."
docker load -i gemma3n-ollama.tar

# 2. ëª¨ë¸ íŒŒì¼ ë³µì›
echo "2. ëª¨ë¸ íŒŒì¼ ë³µì› ì¤‘..."
docker-compose up -d ollama
sleep 5
docker exec gemma3n-ollama mkdir -p /root/.ollama
docker cp models-backup.tar.gz gemma3n-ollama:/tmp/
docker exec gemma3n-ollama tar -xzf /tmp/models-backup.tar.gz -C /

# 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
echo "3. ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘..."
docker-compose restart ollama

# 4. í—¬ìŠ¤ì²´í¬
echo "4. í—¬ìŠ¤ì²´í¬ ì¤‘..."
sleep 10
curl -f http://localhost:11434/api/tags || exit 1

echo "=== ë°°í¬ ì™„ë£Œ! ==="
```

## ğŸ”Œ API ì‚¬ìš©ë²•

### í…ìŠ¤íŠ¸ ìƒì„±

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3n:e4b",
  "prompt": "ì•ˆë…•í•˜ì„¸ìš”. ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
  "stream": false
}'
```

### ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸

```bash
# ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
IMAGE_BASE64=$(base64 -i image.jpg)

curl http://localhost:11434/api/generate -d '{
  "model": "gemma3n:e4b",
  "prompt": "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
  "images": ["'$IMAGE_BASE64'"],
  "stream": false
}'
```

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```bash
curl -N http://localhost:11434/api/generate -d '{
  "model": "gemma3n:e4b",
  "prompt": "ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”.",
  "stream": true
}'
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í•­ëª©
- ì‘ë‹µ ì‹œê°„ (P50, P95, P99)
- ì²˜ë¦¬ëŸ‰ (requests/sec)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- GPU ì‚¬ìš©ë¥  (if available)
- ëª¨ë¸ ë¡œë”© ì‹œê°„
- ì—ëŸ¬ìœ¨

### Prometheus ì„¤ì •

```yaml
# ai-model/monitoring/prometheus.yml
scrape_configs:
  - job_name: 'ollama'
    static_configs:
      - targets: ['ollama:11434']
    metrics_path: '/metrics'
```

## ğŸ”§ ìµœì í™” íŒ

### ë©”ëª¨ë¦¬ ìµœì í™”
1. **ëª¨ë¸ ì–¸ë¡œë“œ**: `OLLAMA_KEEP_ALIVE=5m` ì„¤ì •ìœ¼ë¡œ ë¯¸ì‚¬ìš© ì‹œ ìë™ ì–¸ë¡œë“œ
2. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
3. **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: ë¶ˆí•„ìš”í•œ íˆìŠ¤í† ë¦¬ ì œê±°

### ì„±ëŠ¥ ìµœì í™”
1. **GPU í™œìš©**: NVIDIA GPU ì‚¬ìš© ì‹œ 3-5ë°° ì„±ëŠ¥ í–¥ìƒ
2. **ëª¨ë¸ ì„ íƒ**: ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìµœì†Œ ëª¨ë¸ ì‚¬ìš©
3. **ìºì‹±**: ìì£¼ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìºì‹±

### ì•ˆì •ì„± í™•ë³´
1. **í—¬ìŠ¤ì²´í¬**: 30ì´ˆë§ˆë‹¤ ëª¨ë¸ ìƒíƒœ í™•ì¸
2. **ìë™ ì¬ì‹œì‘**: ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œì‘
3. **ë¦¬ì†ŒìŠ¤ ì œí•œ**: ë©”ëª¨ë¦¬/CPU ì œí•œ ì„¤ì •

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë³´ì•ˆ
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ `OLLAMA_ORIGINS` ì œí•œ í•„ìˆ˜
- API í‚¤ ë˜ëŠ” ì¸ì¦ ë ˆì´ì–´ ì¶”ê°€ ê¶Œì¥
- ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ í™˜ê²½ êµ¬ì„±

### ë¼ì´ì„ ìŠ¤
- Gemma ëª¨ë¸ì€ Gemma Terms of Use ì¤€ìˆ˜ í•„ìš”
- ìƒì—…ì  ì‚¬ìš© ì‹œ ë¼ì´ì„ ìŠ¤ í™•ì¸ í•„ìˆ˜

### ì œí•œì‚¬í•­
- í˜„ì¬ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì…ë ¥ì€ ë¯¸ì§€ì› (ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •)
- ë™ì‹œ ì²˜ë¦¬ ìš”ì²­ ìˆ˜ ì œí•œ (OLLAMA_NUM_PARALLEL)
- ë‹¨ì¼ ëª¨ë¸ ë¡œë”© ì œí•œ (OLLAMA_MAX_LOADED_MODELS)