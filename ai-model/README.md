# AI Model - Gemma 3n with Ollama

ChatGemma í”„ë¡œì íŠ¸ì˜ AI ëª¨ë¸ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. Googleì˜ ìµœì‹  Gemma 3n ëª¨ë¸ì„ Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ê°œë°œ í™˜ê²½ ì‹¤í–‰

```bash
# 1. Docker ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker compose up -d ollama

# 2. Gemma 3n ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìë™)
./scripts/setup.sh

# 3. API í…ŒìŠ¤íŠ¸
./scripts/test-api.sh
```

### ì ‘ì† ì •ë³´

- **Ollama API**: http://localhost:11434
- **ëª¨ë¸**: gemma3n:e4b (6.9B parameters)
- **í—¬ìŠ¤ì²´í¬**: `curl http://localhost:11434/api/tags`

## ğŸ“Š ëª¨ë¸ ì •ë³´

### Gemma 3n E4B ìƒì„¸

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ëª…** | gemma3n:e4b |
| **ì‹¤ì œ í¬ê¸°** | 8B parameters |
| **íš¨ê³¼ì  í¬ê¸°** | 4B parameters (MatFormer) |
| **íŒŒì¼ í¬ê¸°** | 7.5GB (Q4_K_M ì–‘ìí™”) |
| **ë©”ëª¨ë¦¬ ìš”êµ¬** | 8GB RAM ê¶Œì¥ |
| **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´** | 128K tokens |
| **ì–¸ì–´ ì§€ì›** | 140ê°œ ì–¸ì–´ (í•œêµ­ì–´ í¬í•¨) |
| **ë©€í‹°ëª¨ë‹¬** | í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì…ë ¥ |

### ì„±ëŠ¥ íŠ¹ì§•

- **MatFormer ì•„í‚¤í…ì²˜**: ë™ì  íŒŒë¼ë¯¸í„° í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
- **PLE (Per-Layer Embeddings)**: ì‹¤ì œ 8Bì´ì§€ë§Œ 4B ìˆ˜ì¤€ ë©”ëª¨ë¦¬ ì‚¬ìš©
- **ë””ë°”ì´ìŠ¤ ìµœì í™”**: ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤ìš©ìœ¼ë¡œ ì„¤ê³„
- **ì˜¤í”„ë¼ì¸ ì™„ì „ ì§€ì›**: ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”

## ğŸ³ Docker êµ¬ì„±

### ê°œë°œìš© (í˜„ì¬ ì„¤ì •)

```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama/models
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
```

**íŠ¹ì§•:**
- ëª¨ë¸ì„ ëŸ°íƒ€ì„ì— ë‹¤ìš´ë¡œë“œ
- ê°œë°œ ë° í…ŒìŠ¤íŠ¸ìš©
- ë³¼ë¥¨ìœ¼ë¡œ ëª¨ë¸ ë°ì´í„° ìœ ì§€

### ìš´ì˜ìš© (íì‡„ë§ ë°°í¬)

```dockerfile
# Dockerfile.production
FROM ollama/ollama:latest

# ëª¨ë¸ íŒŒì¼ì„ ì´ë¯¸ì§€ì— ì§ì ‘ í¬í•¨
COPY models/ /root/.ollama/models/

# ì‚¬ì „ ë¡œë”©ëœ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
ENV OLLAMA_MODELS=/root/.ollama/models

CMD ["serve"]
```

**íŠ¹ì§•:**
- ëª¨ë¸ì´ ì´ë¯¸ì§€ì— í¬í•¨ë˜ì–´ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¶ˆí•„ìš”
- íì‡„ë§ í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš©

## ğŸ”’ íì‡„ë§ ë°°í¬ ì „ëµ

### 1ë‹¨ê³„: ê°œë°œ í™˜ê²½ì—ì„œ ì¤€ë¹„

```bash
# ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
./scripts/prepare-offline.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…:
1. Gemma 3n ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦
2. ëª¨ë¸ íŒŒì¼ì„ ë¡œì»¬ ë””ë ‰í† ë¦¬ë¡œ ì¶”ì¶œ
3. ìš´ì˜ìš© Docker ì´ë¯¸ì§€ ë¹Œë“œ (ëª¨ë¸ í¬í•¨)
4. ì´ë¯¸ì§€ë¥¼ tar íŒŒì¼ë¡œ ì €ì¥

### 2ë‹¨ê³„: íì‡„ë§ìœ¼ë¡œ ì „ì†¡

```bash
# ìƒì„±ëœ íŒ¨í‚¤ì§€ë¥¼ íì‡„ë§ìœ¼ë¡œ ì „ì†¡
# gemma3n-offline-package.tar.gz (ì•½ 8GB)
```

íŒ¨í‚¤ì§€ êµ¬ì„±:
- `gemma3n-production.tar` - ëª¨ë¸ í¬í•¨ Docker ì´ë¯¸ì§€
- `docker-compose.prod.yml` - ìš´ì˜ í™˜ê²½ ì„¤ì •
- `deploy.sh` - ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
- `health-check.sh` - ë™ì‘ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

### 3ë‹¨ê³„: íì‡„ë§ì—ì„œ ë°°í¬

```bash
# íŒ¨í‚¤ì§€ ì••ì¶• í•´ì œ
tar -xzf gemma3n-offline-package.tar.gz

# Docker ì´ë¯¸ì§€ ë¡œë“œ
docker load -i gemma3n-production.tar

# ì„œë¹„ìŠ¤ ì‹œì‘
docker compose -f docker-compose.prod.yml up -d

# ë™ì‘ í™•ì¸
./health-check.sh
```

## ğŸ”Œ API ì‚¬ìš©ë²•

### ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3n:e4b",
    "prompt": "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
    "stream": false
  }'
```

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3n:e4b",
    "prompt": "ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”.",
    "stream": true
  }'
```

### ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ (ë©€í‹°ëª¨ë‹¬)

```bash
# ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
IMAGE_BASE64=$(base64 -i image.jpg)

curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3n:e4b",
    "prompt": "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "images": ["'$IMAGE_BASE64'"]
  }'
```

### ì‘ë‹µ í¬ë§·

```json
{
  "model": "gemma3n:e4b",
  "created_at": "2025-09-14T14:16:22.979Z",
  "response": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Gemma...",
  "done": true,
  "total_duration": 19370014884,
  "eval_count": 89
}
```

## ğŸ› ï¸ ê°œë°œ ë° ìš´ì˜

### ë¡œì»¬ ê°œë°œ

```bash
# í™˜ê²½ ì„¤ì •
./scripts/setup.sh

# ëª¨ë¸ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
./scripts/benchmark.sh

# ë¡œê·¸ í™•ì¸
docker compose logs -f ollama
```

### ìš´ì˜ í™˜ê²½ ëª¨ë‹ˆí„°ë§

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ
docker compose ps

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
docker stats gemma3n-ollama

# API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸
time curl -X POST http://localhost:11434/api/generate -d '...'
```

### ë¬¸ì œ í•´ê²°

#### ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•Šì„ ë•Œ
```bash
# 1. ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker compose restart ollama

# 2. ëª¨ë¸ ë¦¬ë¡œë”©
docker exec gemma3n-ollama ollama stop gemma3n:e4b
docker exec gemma3n-ollama ollama run gemma3n:e4b "test"

# 3. ë¡œê·¸ í™•ì¸
docker compose logs ollama
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```bash
# Docker ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •
# docker-compose.ymlì—ì„œ memory ì„¤ì • ë³€ê²½
deploy:
  resources:
    limits:
      memory: 16G  # 8Gì—ì„œ 16Gë¡œ ì¦ê°€
```

#### ëŠë¦° ì‘ë‹µ ì†ë„
```bash
# GPU ì‚¬ìš© í™•ì¸ (NVIDIA GPU ìˆëŠ” ê²½ìš°)
docker run --rm --gpus all nvidia/cuda nvidia-smi

# CPU ì „ìš© ìµœì í™”
export OLLAMA_NUM_PARALLEL=4  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •
```

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
ai-model/
â”œâ”€â”€ README.md              # ğŸ“– ì´ ë¬¸ì„œ
â”œâ”€â”€ CLAUDE.md              # ğŸ¤– ìƒì„¸ ê¸°ìˆ  ë¬¸ì„œ
â”œâ”€â”€ docker-compose.yml     # ğŸ³ ê°œë°œí™˜ê²½ Docker êµ¬ì„±
â”œâ”€â”€ docker-compose.prod.yml # ğŸš€ ìš´ì˜í™˜ê²½ Docker êµ¬ì„±
â”œâ”€â”€ Dockerfile.production  # ğŸ—ï¸ ìš´ì˜ìš© ì´ë¯¸ì§€ ë¹Œë“œ
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ download.sh    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
â”‚       â””â”€â”€ health.sh      # í—¬ìŠ¤ì²´í¬
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh           # ê°œë°œí™˜ê²½ ì„¤ì •
â”‚   â”œâ”€â”€ test-api.sh        # API í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ prepare-offline.sh # ì˜¤í”„ë¼ì¸ íŒ¨í‚¤ì§•
â”‚   â”œâ”€â”€ deploy.sh          # íì‡„ë§ ë°°í¬
â”‚   â””â”€â”€ benchmark.sh       # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ config/
â”‚   â””â”€â”€ models.json        # ëª¨ë¸ ì„¤ì •
â””â”€â”€ monitoring/            # ëª¨ë‹ˆí„°ë§ (ì„ íƒì‚¬í•­)
    â”œâ”€â”€ prometheus.yml
    â””â”€â”€ grafana/
```

## ğŸ”§ ì„¤ì • ë° íŠœë‹

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `OLLAMA_HOST` | `127.0.0.1` | ë°”ì¸ë”© í˜¸ìŠ¤íŠ¸ |
| `OLLAMA_ORIGINS` | `localhost` | CORS í—ˆìš© ë„ë©”ì¸ |
| `OLLAMA_NUM_PARALLEL` | `2` | ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ |
| `OLLAMA_MAX_LOADED_MODELS` | `1` | ìµœëŒ€ ë¡œë”© ëª¨ë¸ ìˆ˜ |
| `OLLAMA_KEEP_ALIVE` | `5m` | ëª¨ë¸ ë©”ëª¨ë¦¬ ìœ ì§€ ì‹œê°„ |

### ì„±ëŠ¥ íŠœë‹

```yaml
# docker-compose.yml
services:
  ollama:
    environment:
      # CPU ìµœì í™”
      - OLLAMA_NUM_PARALLEL=4
      - OMP_NUM_THREADS=8

      # ë©”ëª¨ë¦¬ ê´€ë¦¬
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=10m

      # GPU ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## ğŸš¨ ì¤‘ìš” ì‚¬í•­

### ë³´ì•ˆ
- ìš´ì˜ í™˜ê²½ì—ì„œëŠ” `OLLAMA_ORIGINS` ì œí•œ í•„ìˆ˜
- API ì¸ì¦ ë ˆì´ì–´ ì¶”ê°€ ê¶Œì¥ (nginx, API Gateway ë“±)
- ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ ì„¤ì •

### ë¼ì´ì„ ìŠ¤
- Gemma ëª¨ë¸: [Gemma Terms of Use](https://ai.google.dev/gemma/terms) ì¤€ìˆ˜
- Ollama: Apache 2.0 License
- ìƒì—…ì  ì‚¬ìš© ì‹œ ë¼ì´ì„ ìŠ¤ í™•ì¸ í•„ìˆ˜

### ì œí•œì‚¬í•­
- í˜„ì¬ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì…ë ¥ ë¯¸ì§€ì› (ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •)
- ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì œí•œ
- ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

---

**ë‹¤ìŒ ë‹¨ê³„**: [ë°±ì—”ë“œ ì—°ë™ ê°€ì´ë“œ](../backend/CLAUDE.md)ë¥¼ ì°¸ì¡°í•˜ì—¬ Spring Bootì™€ ì—°ë™í•˜ì„¸ìš”.